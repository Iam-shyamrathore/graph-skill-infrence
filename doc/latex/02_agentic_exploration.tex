\section{Agentic Graph Exploration Strategy}
\label{sec:agentic_exploration}

\subsection{Problem Formulation: Graph Traversal as MDP}
We formulate the skill discovery process as a \textbf{Markov Decision Process (MDP)} following the \textbf{DeepPath} framework \cite{DeepPath2017}:
\begin{itemize}
    \item \textbf{State ($s_t$):} A representation $k_t = [e_t, e_{source}, B_t]$, where $e_t$ is the current entity embedding and $B_t$ is the current belief vector.
    \item \textbf{Action ($a_t$):} Selecting a relation $r$ (meta-path step).
    \item \subsection{Multi-Faceted Reward Shaping}
To optimize for high-information paths, we define a composite reward function following the \textbf{DeepPath} framework:
\begin{equation}
R(p) = \alpha \cdot R_{accuracy} + \beta \cdot R_{efficiency} + \gamma \cdot R_{diversity}
\end{equation}
Where:
\begin{itemize}
    \item $R_{accuracy}$ is the confidence score generated by the LLM simulation.
    \item $R_{efficiency} = \frac{1}{\text{length}(p)}$ penalizes excessively long search paths.
    \item $R_{diversity} = 1 - sim(\vec{B}_t, \vec{B}_{t-1})$ rewards the discovery of information that deviates from the current global belief vector $\vec{B}$.
\end{itemize}
\end{itemize}

\subsection{Algorithm: MCTS for Reasoning with Trees (RwT)}
We adopt the \textbf{Reasoning with Trees (RwT)} paradigm which utilizes MCTS to iteratively refine reasoning paths.

\subsubsection{Path-Aware Selection}
Nodes are selected using \textbf{PUCT} (Predictor + UCB). Unlike standard MCTS, our predictor $P(s,a)$ is informed by both structural TF-IDF weights and repository metadata (Stars, Topics).
\begin{equation}
PUCT(s, a) = Q(s, a) + C \cdot P(s, a) \cdot \frac{\sqrt{\Sigma N}}{1 + N(s, a)}
\end{equation}

\subsubsection{Causal Simulation (CoT)}
Instead of generic inference, the simulation phase utilizes \textbf{Chain-of-Thought (CoT)} prompting. The LLM is provided with the full path context $p_{0 \to t}$ to explain the causal relationship between developer actions and latent skill acquisition.

\subsection{Active Learning Criteria}
To minimize expensive LLM calls, we focus exploration on nodes maximizing \textbf{Information Gain} while pruning branches where the **Belief** (via DST) has already converged.
